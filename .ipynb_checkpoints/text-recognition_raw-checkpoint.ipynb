{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60bc815e",
   "metadata": {},
   "source": [
    "Model to recognise hand-written text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bca0464",
   "metadata": {},
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2b8c64",
   "metadata": {},
   "source": [
    "pip install stow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15f1a13",
   "metadata": {},
   "source": [
    "pip install mltu==0.1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "862e951d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libaries\n",
    "import tensorflow as tf\n",
    "import stow\n",
    "import tarfile\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76992789",
   "metadata": {},
   "source": [
    "# Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9ad67f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115338/115338 [00:06<00:00, 18625.32it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_path = r'C:\\Users\\ljant\\Downloads\\IAM_Words\\IAM_Words'\n",
    "\n",
    "# Initialize the dataset list and vocabulary set\n",
    "dataset = []\n",
    "vocab = set()\n",
    "max_len = 0\n",
    "\n",
    "# Path to the words.txt file\n",
    "words_file_path = os.path.join(dataset_path, \"words.txt\")\n",
    "\n",
    "# Reading lines from words.txt\n",
    "with open(words_file_path, \"r\") as file:\n",
    "    words = file.readlines()\n",
    "\n",
    "for line in tqdm(words):\n",
    "    if line.startswith(\"#\"):\n",
    "        continue\n",
    "\n",
    "    line_split = line.split(\" \")\n",
    "    if line_split[1] == \"err\":\n",
    "        continue\n",
    "\n",
    "    folder1 = line_split[0][:3]\n",
    "    folder2 = line_split[0][:8]\n",
    "    file_name = line_split[0] + \".png\"\n",
    "    label = line_split[-1].rstrip('\\n')\n",
    "\n",
    "    # Constructing the relative path to the image\n",
    "    rel_path = os.path.join(dataset_path, \"words\", folder1, folder2, file_name)\n",
    "\n",
    "    # Check if the image file exists\n",
    "    if not os.path.exists(rel_path):\n",
    "        continue\n",
    "\n",
    "    # Append the relative path and label to the dataset list\n",
    "    dataset.append([rel_path, label])\n",
    "    \n",
    "    # Update the vocabulary set with characters from the label\n",
    "    vocab.update(list(label))\n",
    "    \n",
    "    # Update the maximum label length\n",
    "    max_len = max(max_len, len(label))\n",
    "\n",
    "# Now, `dataset` is a list of [image_path, label] and `vocab` contains all unique characters in the labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "642d0cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting configs.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile configs.py\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from mltu.configs import BaseModelConfigs\n",
    "\n",
    "class ModelConfigs(BaseModelConfigs):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model_path = os.path.join(\"C:\\Users\\ljant\\Desktop\\Ironhack\\Projects\\Final-Project-Ironhack-2024\", datetime.strftime(datetime.now(), \"%Y%m%d%H%M\"))\n",
    "        self.vocab = \"\"\n",
    "        self.height = 32\n",
    "        self.width = 128\n",
    "        self.max_text_length = 0\n",
    "        self.batch_size = 16\n",
    "        self.learning_rate = 0.0005\n",
    "        self.train_epochs = 1000\n",
    "        self.train_workers = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cddaa44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs import ModelConfigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f06b531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = ModelConfigs()\n",
    "\n",
    "configs.vocab = \"\".join(vocab)\n",
    "configs.max_text_length = max_len\n",
    "configs.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eab223f",
   "metadata": {},
   "source": [
    "# Create a data provider for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a00b075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataProvider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataProvider.py\n",
    "import os\n",
    "import copy\n",
    "import typing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from .augmentors import Augmentor\n",
    "from .transformers import Transformer\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format=\"%(asctime)s %(levelname)s %(name)s: %(message)s\")\n",
    "\n",
    "\n",
    "class DataProvider:\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset: typing.Union[str, list, pd.DataFrame],\n",
    "            data_preprocessors: typing.List[typing.Callable] = None,\n",
    "            batch_size: int = 4,\n",
    "            shuffle: bool = True,\n",
    "            initial_epoch: int = 1,\n",
    "            augmentors: typing.List[Augmentor] = None,\n",
    "            transformers: typing.List[Transformer] = None,\n",
    "            skip_validation: bool = True,\n",
    "            limit: int = None,\n",
    "            use_cache: bool = False,\n",
    "            log_level: int = logging.INFO,\n",
    "    ) -> None:\n",
    "        \"\"\" Standardised object for providing data to a model while training.\n",
    "\n",
    "        Attributes:\n",
    "            dataset (str, list, pd.DataFrame): Path to dataset, list of data or pandas dataframe of data.\n",
    "            data_preprocessors (list): List of data preprocessors. (e.g. [read image, read audio, etc.])\n",
    "            batch_size (int): The number of samples to include in each batch. Defaults to 4.\n",
    "            shuffle (bool): Whether to shuffle the data. Defaults to True.\n",
    "            initial_epoch (int): The initial epoch. Defaults to 1.\n",
    "            augmentors (list, optional): List of augmentor functions. Defaults to None.\n",
    "            transformers (list, optional): List of transformer functions. Defaults to None.\n",
    "            skip_validation (bool, optional): Whether to skip validation. Defaults to True.\n",
    "            limit (int, optional): Limit the number of samples in the dataset. Defaults to None.\n",
    "            use_cache (bool, optional): Whether to cache the dataset. Defaults to False.\n",
    "            log_level (int, optional): The log level. Defaults to logging.INFO.\n",
    "        \"\"\"\n",
    "        self._dataset = dataset\n",
    "        self._data_preprocessors = [] if data_preprocessors is None else data_preprocessors\n",
    "        self._batch_size = batch_size\n",
    "        self._shuffle = shuffle\n",
    "        self._epoch = initial_epoch\n",
    "        self._augmentors = [] if augmentors is None else augmentors\n",
    "        self._transformers = [] if transformers is None else transformers\n",
    "        self._skip_validation = skip_validation\n",
    "        self._limit = limit\n",
    "        self._use_cache = use_cache\n",
    "        self._step = 0\n",
    "        self._cache = {}\n",
    "        self._on_epoch_end_remove = []\n",
    "\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.setLevel(log_level)\n",
    "\n",
    "        # Validate dataset\n",
    "        if not skip_validation:\n",
    "            self._dataset = self.validate(dataset)\n",
    "        else:\n",
    "            self.logger.info(\"Skipping Dataset validation...\")\n",
    "\n",
    "        if limit:\n",
    "            self.logger.info(f\"Limiting dataset to {limit} samples.\")\n",
    "            self._dataset = self._dataset[:limit]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Denotes the number of batches per epoch \"\"\"\n",
    "        return int(np.ceil(len(self._dataset) / self._batch_size))\n",
    "\n",
    "    @property\n",
    "    def augmentors(self) -> typing.List[Augmentor]:\n",
    "        \"\"\" Return augmentors \"\"\"\n",
    "        return self._augmentors\n",
    "\n",
    "    @augmentors.setter\n",
    "    def augmentors(self, augmentors: typing.List[Augmentor]):\n",
    "        \"\"\" Decorator for adding augmentors to the DataProvider \"\"\"\n",
    "        for augmentor in augmentors:\n",
    "            if isinstance(augmentor, Augmentor):\n",
    "                if self._augmentors is not None:\n",
    "                    self._augmentors.append(augmentor)\n",
    "                else:\n",
    "                    self._augmentors = [augmentor]\n",
    "\n",
    "            else:\n",
    "                self.logger.warning(f\"Augmentor {augmentor} is not an instance of Augmentor.\")\n",
    "\n",
    "    @property\n",
    "    def transformers(self) -> typing.List[Transformer]:\n",
    "        \"\"\" Return transformers \"\"\"\n",
    "        return self._transformers\n",
    "\n",
    "    @transformers.setter\n",
    "    def transformers(self, transformers: typing.List[Transformer]):\n",
    "        \"\"\" Decorator for adding transformers to the DataProvider \"\"\"\n",
    "        for transformer in transformers:\n",
    "            if isinstance(transformer, Transformer):\n",
    "                if self._transformers is not None:\n",
    "                    self._transformers.append(transformer)\n",
    "                else:\n",
    "                    self._transformers = [transformer]\n",
    "\n",
    "            else:\n",
    "                self.logger.warning(f\"Transformer {transformer} is not an instance of Transformer.\")\n",
    "\n",
    "    @property\n",
    "    def epoch(self) -> int:\n",
    "        \"\"\" Return Current Epoch\"\"\"\n",
    "        return self._epoch\n",
    "\n",
    "    @property\n",
    "    def step(self) -> int:\n",
    "        \"\"\" Return Current Step\"\"\"\n",
    "        return self._step\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\" Shuffle training dataset and increment epoch counter at the end of each epoch. \"\"\"\n",
    "        self._epoch += 1\n",
    "        if self._shuffle:\n",
    "            np.random.shuffle(self._dataset)\n",
    "\n",
    "        # Remove any samples that were marked for removal\n",
    "        for remove in self._on_epoch_end_remove:\n",
    "            self.logger.warning(f\"Removing {remove} from dataset.\")\n",
    "            self._dataset.remove(remove)\n",
    "        self._on_epoch_end_remove = []\n",
    "\n",
    "    def validate_list_dataset(self, dataset: list) -> list:\n",
    "        \"\"\" Validate a list dataset \"\"\"\n",
    "        validated_data = [data for data in tqdm(dataset, desc=\"Validating Dataset\") if os.path.exists(data[0])]\n",
    "        if not validated_data:\n",
    "            raise FileNotFoundError(\"No valid data found in dataset.\")\n",
    "\n",
    "        return validated_data\n",
    "\n",
    "    def validate(self, dataset: typing.Union[str, list, pd.DataFrame]) -> typing.Union[list, str]:\n",
    "        \"\"\" Validate the dataset and return the dataset \"\"\"\n",
    "\n",
    "        if isinstance(dataset, str):\n",
    "            if os.path.exists(dataset):\n",
    "                return dataset\n",
    "        elif isinstance(dataset, list):\n",
    "            return self.validate_list_dataset(dataset)\n",
    "        elif isinstance(dataset, pd.DataFrame):\n",
    "            return self.validate_list_dataset(dataset.values.tolist())\n",
    "        else:\n",
    "            raise TypeError(\"Dataset must be a path, list or pandas dataframe.\")\n",
    "\n",
    "    def split(self, split: float = 0.9, shuffle: bool = True) -> typing.Tuple[typing.Any, typing.Any]:\n",
    "        \"\"\" Split current data provider into training and validation data providers. \n",
    "        \n",
    "        Args:\n",
    "            split (float, optional): The split ratio. Defaults to 0.9.\n",
    "            shuffle (bool, optional): Whether to shuffle the dataset. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            train_data_provider (tf.keras.utils.Sequence): The training data provider.\n",
    "            val_data_provider (tf.keras.utils.Sequence): The validation data provider.\n",
    "        \"\"\"\n",
    "        if shuffle:\n",
    "            np.random.shuffle(self._dataset)\n",
    "            \n",
    "        train_data_provider, val_data_provider = copy.deepcopy(self), copy.deepcopy(self)\n",
    "        train_data_provider._dataset = self._dataset[:int(len(self._dataset) * split)]\n",
    "        val_data_provider._dataset = self._dataset[int(len(self._dataset) * split):]\n",
    "\n",
    "        return train_data_provider, val_data_provider\n",
    "\n",
    "    def to_csv(self, path: str, index: bool = False) -> None:\n",
    "        \"\"\" Save the dataset to a csv file \n",
    "\n",
    "        Args:\n",
    "            path (str): The path to save the csv file.\n",
    "            index (bool, optional): Whether to save the index. Defaults to False.\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame(self._dataset)\n",
    "        df.to_csv(path, index=index)\n",
    "\n",
    "    def get_batch_annotations(self, index: int) -> typing.List:\n",
    "        \"\"\" Returns a batch of annotations by batch index in the dataset\n",
    "\n",
    "        Args:\n",
    "            index (int): The index of the batch in \n",
    "\n",
    "        Returns:\n",
    "            batch_annotations (list): A list of batch annotations\n",
    "        \"\"\"\n",
    "        self._step = index\n",
    "        start_index = index * self._batch_size\n",
    "\n",
    "        # Get batch indexes\n",
    "        batch_indexes = [i for i in range(start_index, start_index + self._batch_size) if i < len(self._dataset)]\n",
    "\n",
    "        # Read batch data\n",
    "        batch_annotations = [self._dataset[index] for index in batch_indexes]\n",
    "\n",
    "        return batch_annotations\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\" Create a generator that iterate over the Sequence.\"\"\"\n",
    "        for item in (self[i] for i in range(len(self))):\n",
    "            yield item\n",
    "\n",
    "    def process_data(self, batch_data):\n",
    "        \"\"\" Process data batch of data \"\"\"\n",
    "        if self._use_cache and batch_data[0] in self._cache:\n",
    "            data, annotation = copy.deepcopy(self._cache[batch_data[0]])\n",
    "        else:\n",
    "            data, annotation = batch_data\n",
    "            for preprocessor in self._data_preprocessors:\n",
    "                data, annotation = preprocessor(data, annotation)\n",
    "            \n",
    "            if data is None or annotation is None:\n",
    "                self.logger.warning(\"Data or annotation is None, marking for removal on epoch end.\")\n",
    "                self._on_epoch_end_remove.append(batch_data)\n",
    "                return None, None\n",
    "            \n",
    "            if self._use_cache and batch_data[0] not in self._cache:\n",
    "                self._cache[batch_data[0]] = (copy.deepcopy(data), copy.deepcopy(annotation))\n",
    "\n",
    "        # Then augment, transform and postprocess the batch data\n",
    "        for objects in [self._augmentors, self._transformers]:\n",
    "            for _object in objects:\n",
    "                data, annotation = _object(data, annotation)\n",
    "\n",
    "        # Convert to numpy array if not already\n",
    "        if not isinstance(data, np.ndarray):\n",
    "            data = data.numpy()\n",
    "\n",
    "        # Convert to numpy array if not already\n",
    "        # TODO: This is a hack, need to fix this\n",
    "        if not isinstance(annotation, (np.ndarray, int, float, str, np.uint8, float)):\n",
    "            annotation = annotation.numpy()\n",
    "\n",
    "        return data, annotation\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\" Returns a batch of data by batch index\"\"\"\n",
    "        dataset_batch = self.get_batch_annotations(index)\n",
    "        \n",
    "        # First read and preprocess the batch data\n",
    "        batch_data, batch_annotations = [], []\n",
    "        for index, batch in enumerate(dataset_batch):\n",
    "\n",
    "            data, annotation = self.process_data(batch)\n",
    "\n",
    "            if data is None or annotation is None:\n",
    "                self.logger.warning(\"Data or annotation is None, skipping.\")\n",
    "                continue\n",
    "\n",
    "            batch_data.append(data)\n",
    "            batch_annotations.append(annotation)\n",
    "\n",
    "        return np.array(batch_data), np.array(batch_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6c734340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\ljant\\\\Desktop\\\\Ironhack\\\\Projects\\\\Final-Project-Ironhack-2024', 'C:\\\\Users\\\\ljant\\\\anaconda3\\\\python39.zip', 'C:\\\\Users\\\\ljant\\\\anaconda3\\\\DLLs', 'C:\\\\Users\\\\ljant\\\\anaconda3\\\\lib', 'C:\\\\Users\\\\ljant\\\\anaconda3', '', 'C:\\\\Users\\\\ljant\\\\anaconda3\\\\lib\\\\site-packages', 'C:\\\\Users\\\\ljant\\\\anaconda3\\\\lib\\\\site-packages\\\\locket-0.2.1-py3.9.egg', 'C:\\\\Users\\\\ljant\\\\anaconda3\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\ljant\\\\anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\ljant\\\\anaconda3\\\\lib\\\\site-packages\\\\Pythonwin', 'C:\\\\Users\\\\ljant\\\\anaconda3\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\ljant\\\\.ipython', 'C:\\\\Users\\\\ljant\\\\Desktop\\\\Ironhack\\\\Projects']\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'augmentors'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5476/3813421706.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Try importing again\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0maugmentors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAugmentor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'augmentors'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Print the current search path\n",
    "print(sys.path)\n",
    "\n",
    "# Add a new directory to the search path\n",
    "# Replace '/path/to/your/module' with the actual path to the directory containing your modules\n",
    "module_path = r'C:\\Users\\ljant\\Desktop\\Ironhack\\Projects\\Final-Project-Ironhack-2024\\dataProvider.py'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Try importing again\n",
    "from augmentors import Augmentor\n",
    "from transformers import Transformer\n",
    "\n",
    "data_provider = DataProvider(\n",
    "    dataset=dataset,\n",
    "    skip_validation=True,\n",
    "    batch_size=configs.batch_size,\n",
    "    data_preprocessors=[ImageReader(CVImage)],\n",
    "    transformers=[\n",
    "        ImageResizer(configs.width, configs.height, keep_aspect_ratio=False),\n",
    "        LabelIndexer(configs.vocab),\n",
    "        LabelPadding(max_word_length=configs.max_text_length, padding_value=len(configs.vocab)),\n",
    "        ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75078f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5394b389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b91387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "92aeeeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "\n",
    "from mltu.tensorflow.model_utils import residual_block\n",
    "\n",
    "\n",
    "def train_model(input_dim, output_dim, activation=\"leaky_relu\", dropout=0.2):\n",
    "    \n",
    "    inputs = layers.Input(shape=input_dim, name=\"input\")\n",
    "\n",
    "    # normalize images here instead in preprocessing step\n",
    "    input = layers.Lambda(lambda x: x / 255)(inputs)\n",
    "\n",
    "    x1 = residual_block(input, 16, activation=activation, skip_conv=True, strides=1, dropout=dropout)\n",
    "\n",
    "    x2 = residual_block(x1, 16, activation=activation, skip_conv=True, strides=2, dropout=dropout)\n",
    "    x3 = residual_block(x2, 16, activation=activation, skip_conv=False, strides=1, dropout=dropout)\n",
    "\n",
    "    x4 = residual_block(x3, 32, activation=activation, skip_conv=True, strides=2, dropout=dropout)\n",
    "    x5 = residual_block(x4, 32, activation=activation, skip_conv=False, strides=1, dropout=dropout)\n",
    "\n",
    "    x6 = residual_block(x5, 64, activation=activation, skip_conv=True, strides=2, dropout=dropout)\n",
    "    x7 = residual_block(x6, 64, activation=activation, skip_conv=True, strides=1, dropout=dropout)\n",
    "\n",
    "    x8 = residual_block(x7, 64, activation=activation, skip_conv=False, strides=1, dropout=dropout)\n",
    "    x9 = residual_block(x8, 64, activation=activation, skip_conv=False, strides=1, dropout=dropout)\n",
    "\n",
    "    squeezed = layers.Reshape((x9.shape[-3] * x9.shape[-2], x9.shape[-1]))(x9)\n",
    "\n",
    "    blstm = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(squeezed)\n",
    "    blstm = layers.Dropout(dropout)(blstm)\n",
    "\n",
    "    output = layers.Dense(output_dim + 1, activation=\"softmax\", name=\"output\")(blstm)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91e43ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5601f0cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
